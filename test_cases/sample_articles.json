[{"text": "Bitcoin is a cryptocurrency and worldwide payment system.[8]:3 It is the first decentralized digital currency, as the system works without a central bank or single administrator.[8]:1[9] The network is peer-to-peer and transactions take place between users directly, without an intermediary.[8]:4 These transactions are verified by network nodes through the use of cryptography and recorded in a public distributed ledger called a blockchain. Bitcoin was invented by an unknown person or group of people under the name Satoshi Nakamoto[10] and released as open-source software in 2009.[11]\nBitcoins are created as a reward for a process known as mining. They can be exchanged for other currencies,[12] products, and services. As of February 2015, over 100,000 merchants and vendors accepted bitcoin as payment.[13] Research produced by the University of Cambridge estimates that in 2017, there are 2.9 to 5.8 million unique users using a cryptocurrency wallet, most of them using bitcoin.[14]\n3 Design\n4 Economics\nThe word bitcoin first occurred and was defined in the white paper[15] that was published on 31 October 2008.[16] It is a compound of the words bit and coin.[17] The white paper frequently uses the shorter coin.[15]\nThere is no uniform convention for bitcoin capitalization. Some sources use Bitcoin, capitalized, to refer to the technology and network and bitcoin, lowercase, to refer to the unit of account.[18] The Wall Street Journal,[19] The Chronicle of Higher Education,[20] and the Oxford English Dictionary[17] advocate use of lowercase bitcoin in all cases, a convention followed throughout this article.\nThe unit of account of the bitcoin system is bitcoin. As of 2014, ticker symbols used to represent bitcoin are BTC[a] and XBT.[b] Its Unicode character is \u20bf.[25]:2 Small amounts of bitcoin used as alternative units are millibitcoin (mBTC)[1] and satoshi. Named in homage to bitcoin's creator, a satoshi is the smallest amount within bitcoin representing 0.00000001 bitcoin, one hundred millionth of a bitcoin.[4] A millibitcoin equals 0.001 bitcoin, one thousandth of a bitcoin or 100,000 satoshis.[26]\nOn 18 August 2008, the domain name \"bitcoin.org\" was registered.[27] In November that year, a link to a paper authored by Satoshi Nakamoto titled Bitcoin: A Peer-to-Peer Electronic Cash System[15] was posted to a cryptography mailing list.[27] Nakamoto implemented the bitcoin software as open source code and released it in January 2009.[28][11] The identity of Nakamoto remains unknown.[10]\nIn January 2009, the bitcoin network came into existence after Satoshi Nakamoto mined the first ever block on the chain, known as the genesis block, for a reward of 50 bitcoins.[29][30] Embedded in the coinbase of this block was the following text:\nThe Times 03/Jan/2009 Chancellor on brink of second bailout for banks.[11]\nThis note has been interpreted as both a timestamp of the genesis date and a derisive comment on the instability caused by fractional-reserve banking.[31]\nOne of the first supporters, adopters, and contributors to bitcoin was the receiver of the first bitcoin transaction, programmer Hal Finney. Finney downloaded the bitcoin software the day it was released, and received 10 bitcoins from Nakamoto in the world's first bitcoin transaction.[32][33] Other early supporters were Wei Dai, creator of bitcoin predecessor b-money, and Nick Szabo, creator of bitcoin predecessor bit gold.[34]\nIn the early days, Nakamoto is estimated to have mined 1 million bitcoins.[35] In 2010, Nakamoto handed the network alert key and control of the Bitcoin Core code repository over to Gavin Andresen, who later became lead developer at the Bitcoin Foundation.[36][37] Nakamoto subsequently disappeared from any involvement in bitcoin.[38] Andresen stated he then sought to decentralize control, saying: \"As soon as Satoshi stepped back and threw the project onto my shoulders, one of the first things I did was try to decentralize that. So, if I get hit by a bus, it would be clear that the project would go on.\"[38] This left opportunity for controversy to develop over the future development path of bitcoin.[39]\nThe value of the first bitcoin transactions were negotiated by individuals on the bitcointalk forums with one notable transaction of 10,000 BTC used to indirectly purchase two pizzas delivered by Papa John's.[29]\nOn 6 August 2010, a major vulnerability in the bitcoin protocol was spotted. Transactions were not properly verified before they were included in the blockchain, which let users bypass bitcoin's economic restrictions and create an indefinite number of bitcoins.[40][41] On 15 August, the vulnerability was exploited; over 184 billion bitcoins were generated in a single transaction, and sent to two addresses on the network. Within hours, the transaction was spotted and erased from the transaction log after the bug was fixed and the network forked to an updated version of the bitcoin protocol.[42][40][41]\nOn 1 August 2017, a hard fork of bitcoin was created, known as Bitcoin Cash. Bitcoin Cash has a larger blocksize limit and had an identical blockchain at the time of fork.[43][44] On November 12 another hard fork, Bitcoin Gold, was created. Bitcoin Gold changes the proof-of-work algorithm used in mining.[45][46]\nThe blockchain is a public ledger that records bitcoin transactions.[47] A novel solution accomplishes this without any trusted central authority: the maintenance of the blockchain is performed by a network of communicating nodes running bitcoin software.[8] Transactions of the form payer X sends Y bitcoins to payee Z are broadcast to this network using readily available software applications.[48] Network nodes can validate transactions, add them to their copy of the ledger, and then broadcast these ledger additions to other nodes. The blockchain is a distributed database \u2013 to achieve independent verification of the chain of ownership of any and every bitcoin amount, each network node stores its own copy of the blockchain.[49] Approximately six times per hour, a new group of accepted transactions, a block, is created, added to the blockchain, and quickly published to all nodes. This allows bitcoin software to determine when a particular bitcoin amount has been spent, which is necessary in order to prevent double-spending in an environment without central oversight. Whereas a conventional ledger records the transfers of actual bills or promissory notes that exist apart from it, the blockchain is the only place that bitcoins can be said to exist in the form of unspent outputs of transactions.[3]:ch. 5\nTransactions are defined using a Forth-like scripting language.[3]:ch. 5 Transactions consist of one or more inputs and one or more outputs. When a user sends bitcoins, the user designates each address and the amount of bitcoin being sent to that address in an output. To prevent double spending, each input must refer to a previous unspent output in the blockchain.[51] The use of multiple inputs corresponds to the use of multiple coins in a cash transaction. Since transactions can have multiple outputs, users can send bitcoins to multiple recipients in one transaction. As in a cash transaction, the sum of inputs (coins used to pay) can exceed the intended sum of payments. In such a case, an additional output is used, returning the change back to the payer.[51] Any input satoshis not accounted for in the transaction outputs become the transaction fee.[51]\nPaying a transaction fee is optional.[51] Miners can choose which transactions to process[51] and prioritize those that pay higher fees. Fees are based on the storage size of the transaction generated, which in turn is dependent on the number of inputs used to create the transaction. Furthermore, priority is given to older unspent inputs.[3]:ch. 8\nIn the blockchain, bitcoins are registered to bitcoin addresses. Creating a bitcoin address is nothing more than picking a random valid private key and computing the corresponding bitcoin address. This computation can be done in a split second. But the reverse (computing the private key of a given bitcoin address) is mathematically unfeasible and so users can tell others and make public a bitcoin address without compromising its corresponding private key. Moreover, the number of valid private keys is so vast that it is extremely unlikely someone will compute a key-pair that is already in use and has funds. The vast number of valid private keys makes it unfeasible that brute force could be used for that. To be able to spend the bitcoins, the owner must know the corresponding private key and digitally sign the transaction. The network verifies the signature using the public key.[3]:ch. 5\nIf the private key is lost, the bitcoin network will not recognize any other evidence of ownership;[8] the coins are then unusable, and effectively lost. For example, in 2013 one user claimed to have lost 7,500 bitcoins, worth $7.5 million at the time, when he accidentally discarded a hard drive containing his private key.[52] A backup of his key(s) would have prevented this.[53]\nMining is a record-keeping service done through the use of computer processing power.[d] Miners keep the blockchain consistent, complete, and unalterable by repeatedly verifying and collecting newly broadcast transactions into a new group of transactions called a block.[47] Each block contains a cryptographic hash of the previous block,[47] using the SHA-256 hashing algorithm,[3]:ch. 7 which links it to the previous block,[47] thus giving the blockchain its name.\nTo be accepted by the rest of the network, a new block must contain a so-called proof-of-work.[47] The proof-of-work requires miners to find a number called a nonce, such that when the block content is hashed along with the nonce, the result is numerically smaller than the network's difficulty target.[3]:ch. 8 This proof is easy for any node in the network to verify, but extremely time-consuming to generate, as for a secure cryptographic hash, miners must try many different nonce values (usually the sequence of tested values is 0, 1, 2, 3, ...[3]:ch. 8) before meeting the difficulty target.\nEvery 2,016 blocks (approximately 14 days at roughly 10 min per block), the difficulty target is adjusted based on the network's recent performance, with the aim of keeping the average time between new blocks at ten minutes. In this way the system automatically adapts to the total amount of mining power on the network.[3]:ch. 8\nBetween 1 March 2014 and 1 March 2015, the average number of nonces miners had to try before creating a new block increased from 16.4 quintillion to 200.5 quintillion.[55]\nThe proof-of-work system, alongside the chaining of blocks, makes modifications of the blockchain extremely hard, as an attacker must modify all subsequent blocks in order for the modifications of one block to be accepted.[56] As new blocks are mined all the time, the difficulty of modifying a block increases as time passes and the number of subsequent blocks (also called confirmations of the given block) increases.[47]\nComputing power is often bundled together or \"pooled\" to reduce variance in miner income. Individual mining rigs often have to wait for long periods to confirm a block of transactions and receive payment. In a pool, all participating miners get paid every time a participating server solves a block. This payment depends on the amount of work an individual miner contributed to help find that block.[57]\nThe successful miner finding the new block is rewarded with newly created bitcoins and transaction fees.[58] As of 9 July 2016,[59] the reward amounted to 12.5 newly created bitcoins per block added to the blockchain. To claim the reward, a special transaction called a coinbase is included with the processed payments.[3]:ch. 8 All bitcoins in existence have been created in such coinbase transactions. The bitcoin protocol specifies that the reward for adding a block will be halved every 210,000 blocks (approximately every four years). Eventually, the reward will decrease to zero, and the limit of 21 million bitcoins[e] will be reached c. 2140; the record keeping will then be rewarded by transaction fees solely.[60]\nIn other words, bitcoin's inventor Nakamoto set a monetary policy based on artificial scarcity at bitcoin's inception that there would only ever be 21 million bitcoins in total. Their numbers are being released roughly every ten minutes and the rate at which they are generated would drop by half every four years until all were in circulation.[61]\nA wallet stores the information necessary to transact bitcoins. While wallets are often described as a place to hold[62] or store bitcoins,[63] due to the nature of the system, bitcoins are inseparable from the blockchain transaction ledger. A better way to describe a wallet is something that \"stores the digital credentials for your bitcoin holdings\"[63] and allows one to access (and spend) them. Bitcoin uses public-key cryptography, in which two cryptographic keys, one public and one private, are generated.[64] At its most basic, a wallet is a collection of these keys.\nThere are several types of wallets. Software wallets connect to the network and allow spending bitcoins in addition to holding the credentials that prove ownership.[65] Software wallets can be split further in two categories: full clients and lightweight clients.\nFull clients verify transactions directly on a local copy of the blockchain (over 136 GB as of October 2017),[66] or a subset of the blockchain (around 2 GB).[67][better source needed] They are the most secure and reliable way of using the network, as trust in external parties is not required. Full clients check the validity of mined blocks, preventing them from transacting on a chain that breaks or alters network rules.[68] Because of its size and complexity, storing the entire blockchain is not suitable for all computing devices.\nLightweight clients on the other hand consult a full client to send and receive transactions without requiring a local copy of the entire blockchain (see simplified payment verification \u2013 SPV). This makes lightweight clients much faster to set up and allows them to be used on low-power, low-bandwidth devices such as smartphones. When using a lightweight wallet, however, the user must trust the server to a certain degree, as it can report faulty values back to the user. Lightweight clients follow the longest blockchain and do not ensure it is valid, requiring trust in miners.\nWith both types of software wallets, the users are responsible for keeping their private keys in a secure place.[69]\nBesides software wallets, Internet services called online wallets offer similar functionality but may be easier to use. In this case, credentials to access funds are stored with the online wallet provider rather than on the user's hardware.[70][71] As a result, the user must have complete trust in the wallet provider. A malicious provider or a breach in server security may cause entrusted bitcoins to be stolen. An example of such security breach occurred with Mt. Gox in 2011.[72]\nPhysical wallets store the credentials necessary to spend bitcoins offline.[63] Examples combine a novelty coin with these credentials printed on metal.[73] Paper wallets are simply paper printouts. Another type of wallet called a hardware wallet keeps credentials offline while facilitating transactions.[74]\nThe first wallet program \u2013 simply named \"Bitcoin\" \u2013 was released in 2009 by Satoshi Nakamoto as open-source code.[11] Sometimes referred to as the \"Satoshi client\", this is also known as the reference client because it serves to define the bitcoin protocol and acts as a standard for other implementations.[65] In version 0.5 the client moved from the wxWidgets user interface toolkit to Qt, and the whole bundle was referred to as Bitcoin-Qt.[65] After the release of version 0.9, the software bundle was renamed Bitcoin Core to distinguish itself from the underlying network.[75][76] Today, other forks of Bitcoin Core exist such as Bitcoin XT, Bitcoin Classic, Bitcoin Unlimited,[39][77] Parity Bitcoin,[78] and BTC1.[79]\nBitcoin creator Satoshi Nakamoto designed bitcoin not to need a central authority.[15] According to the academic Mercatus Center,[8] US Treasury,[5] IEEE Communications, Surveys & Tutorials,[9] The Washington Post,[80] The Daily Herald,[81] The New Yorker,[82] and others, bitcoin is decentralized.\nBitcoin is pseudonymous, meaning that funds are not tied to real-world entities but rather bitcoin addresses. Owners of bitcoin addresses are not explicitly identified, but all transactions on the blockchain are public. In addition, transactions can be linked to individuals and companies through \"idioms of use\" (e.g., transactions that spend coins from multiple inputs indicate that the inputs may have a common owner) and corroborating public transaction data with known information on owners of certain addresses.[83] Additionally, bitcoin exchanges, where bitcoins are traded for traditional currencies, may be required by law to collect personal information.[84]\nTo heighten financial privacy, a new bitcoin address can be generated for each transaction.[85] For example, hierarchical deterministic wallets generate pseudorandom \"rolling addresses\" for every transaction from a single seed, while only requiring a single passphrase to be remembered to recover all corresponding private keys.[86] Researchers at Stanford University and Concordia University have also shown that bitcoin exchanges and other entities can prove assets, liabilities, and solvency without revealing their addresses using zero-knowledge proofs.[87]\nWallets and similar software technically handle all bitcoins as equivalent, establishing the basic level of fungibility. Researchers have pointed out that the history of each bitcoin is registered and publicly available in the blockchain ledger, and that some users may refuse to accept bitcoins coming from controversial transactions, which would harm bitcoin's fungibility.[88] Projects such as CryptoNote, Zerocoin, and Dark Wallet aim to address these privacy and fungibility issues.[89][90]\nThe blocks in the blockchain are limited to one megabyte in size, which has created problems for bitcoin transaction processing, such as increasing transaction fees and delayed processing of transactions that cannot be fit into a block.[91] On 24 August 2017 (at block 481,824), Segregated Witness went live, increasing maximum block capacity and making transaction IDs immutable.[92][better source needed][93] SegWit also allows implementation of the Lightning Network, a second-layer proposal for scalability with instantaneous transactions and near-zero fees.[94][95]\nThe question whether bitcoin is a currency or not is still disputed.[99] Bitcoins have three useful qualities in a currency, according to The Economist in January 2015: they are \"hard to earn, limited in supply and easy to verify\".[100] Economists define money as a store of value, a medium of exchange, and a unit of account and agree that bitcoin has some way to go to meet all these criteria.[101] It does best as a medium of exchange; as of February 2015 the number of merchants accepting bitcoin had passed 100,000.[13] As of March 2014, the bitcoin market suffered from volatility, limiting the ability of bitcoin to act as a stable store of value, and retailers accepting bitcoin use other currencies as their principal unit of account.[101]\nAccording to research produced by Cambridge University, there were between 2.9 million and 5.8 million unique users using a cryptocurrency wallet, as of 2017, most of them using bitcoin. The number of users has grown significantly since 2013, when there were 300,000 to 1.3 million users.[14]\nIn 2015, the number of merchants accepting bitcoin exceeded 100,000.[13] Instead of 2\u20133% typically imposed by credit card processors, merchants accepting bitcoins often pay fees under 2%, down to 0%.[102] Firms that accepted payments in bitcoin as of December 2014 included PayPal,[103] Microsoft,[104] Dell,[105] and Newegg.[106] In November 2017 PwC accepted bitcoin at its Hong Kong office in exchange for providing advisory services to local companies who are specialists in blockchain technology and cryptocurrencies, the first time any Big Four accounting firm accepted the cryptocurrency as payment.[107] [108]\nMerchants accepting bitcoin ordinarily use the services of bitcoin payment service providers such as BitPay or Coinbase. When a customer pays in bitcoin, the payment service provider accepts the bitcoin on behalf of the merchant, converts it to the local currency, and sends the obtained amount to merchant's bank account, charging a fee for the service.[109]\nBitcoins can be bought on digital currency exchanges. According to Tony Gallippi, a co-founder of BitPay, \"banks are scared to deal with bitcoin companies, even if they really want to\".[110] In 2014, the National Australia Bank closed accounts of businesses with ties to bitcoin,[111] and HSBC refused to serve a hedge fund with links to bitcoin.[112] Australian banks in general have been reported as closing down bank accounts of operators of businesses involving the currency;[113] this has become the subject of an investigation by the Australian Competition and Consumer Commission.[113] Nonetheless, Australian banks have trialled trading between each other using the blockchain technology on which bitcoin is based.[114]\nIn a 2013 report, Bank of America Merrill Lynch stated that \"we believe bitcoin can become a major means of payment for e-commerce and may emerge as a serious competitor to traditional money-transfer providers.\"[115] In June 2014, the first bank that converts deposits in currencies instantly to bitcoin without any fees was opened in Boston.[116]\nPlans were announced to include a bitcoin futures option on the Chicago Mercantile Exchange in 2017.[117] Trading in bitcoin futures was announced to begin on December 10, 2017.[118]\nSome Argentinians have bought bitcoins to protect their savings against high inflation or the possibility that governments could confiscate savings accounts.[84] During the 2012\u20132013 Cypriot financial crisis, bitcoin purchases in Cyprus rose due to fears that savings accounts would be confiscated or taxed.[119]\nThe Winklevoss twins have invested into bitcoins. In 2013 The Washington Post claimed that they owned 1% of all the bitcoins in existence at the time.[120]\nOther methods of investment are bitcoin funds. The first regulated bitcoin fund was established in Jersey in July 2014 and approved by the Jersey Financial Services Commission.[121] Forbes started publishing arguments in favor of investing in December 2015.[122]\nIn 2013 and 2014, the European Banking Authority[123] and the Financial Industry Regulatory Authority (FINRA), a United States self-regulatory organization,[124] warned that investing in bitcoins carries significant risks. Forbes named bitcoin the best investment of 2013.[125] In 2014, Bloomberg named bitcoin one of its worst investments of the year.[126] In 2015, bitcoin topped Bloomberg's currency tables.[127]\nAccording to bitinfocharts.com, in 2017 there are 9,272 bitcoin wallets with more than $1 million worth of bitcoins.[128] The exact number of bitcoin millionaires is uncertain as a single person can have more than one bitcoin wallet.\nVenture capitalists, such as Peter Thiel's Founders Fund, which invested US$3 million in BitPay, do not purchase bitcoins themselves, instead funding bitcoin infrastructure like companies that provide payment systems to merchants, exchanges, wallet services, etc.[129] In 2012, an incubator for bitcoin-focused start-ups was founded by Adam Draper, with financing help from his father, venture capitalist Tim Draper, one of the largest bitcoin holders after winning an auction of 30,000 bitcoins,[130] at the time called 'mystery buyer'.[131] The company's goal is to fund 100 bitcoin businesses within 2\u20133 years with $10,000 to $20,000 for a 6% stake.[130] Investors also invest in bitcoin mining.[132] According to a 2015 study by Paolo Tasca, bitcoin startups raised almost $1 billion in three years (Q1 2012 \u2013 Q1 2015).[133]\nThe price of bitcoins has gone through various cycles of appreciation and depreciation referred to by some as bubbles and busts.[134][135] In 2011, the value of one bitcoin rapidly rose from about US$0.30 to US$32 before returning to US$2.[136] In the latter half of 2012 and during the 2012\u201313 Cypriot financial crisis, the bitcoin price began to rise,[137] reaching a high of US$266 on 10 April 2013, before crashing to around US$50.[138] On 29 November 2013, the cost of one bitcoin rose to a peak of US$1,242.[139] In 2014, the price fell sharply, and as of April remained depressed at little more than half 2013 prices. As of August 2014 it was under US$600.[140]\nAccording to Mark T. Williams, as of 2014, bitcoin has volatility seven times greater than gold, eight times greater than the S&P 500, and 18 times greater than the US dollar.[141] According to Forbes, there are uses where volatility does not matter, such as online gambling, tipping, and international remittances.[142]\nIn January 2015, noting that the bitcoin price had dropped to its lowest level since spring 2013 \u2013 around US$224 \u2013 The New York Times suggested that \"[w]ith no signs of a rally in the offing, the industry is bracing for the effects of a prolonged decline in prices. In particular, bitcoin mining companies, which are essential to the currency's underlying technology, are flashing warning signs.\"[143] Also in January 2015, Business Insider reported that deep web drug dealers were \"freaking out\" as they lost profits through being unable to convert bitcoin revenue to cash quickly enough as the price declined \u2013 and that there was a danger that dealers selling reserves to stay in business might force the bitcoin price down further.[144]\nAccording to an article in The Wall Street Journal, as of 19 April 2016, bitcoin had been more stable than gold for the preceding 24 days, and it was suggested that its value might be more stable in the future.[145] On 3 March 2017, the price of a bitcoin surpassed the market value of an ounce of gold for the first time as its price surged to an all-time high of $1,268.[146][147] A study in Electronic Commerce Research and Applications, going back through the network's historical data, showed the value of the bitcoin network as measured by the price of bitcoins, to be roughly proportional to the square of the number of daily unique users participating on the network, i.e. that the network is \"fairly well modeled by the Metcalfe's law\".[148]\nVarious journalists,[81][149] economists,[150][151] and the central bank of Estonia[152] have voiced concerns that bitcoin is a Ponzi scheme. In 2013, Eric Posner, a law professor at the University of Chicago, stated that \"a real Ponzi scheme takes fraud; bitcoin, by contrast, seems more like a collective delusion.\"[153] A 2014 report by the World Bank concluded that bitcoin was not a deliberate Ponzi scheme.[154]:7 The Swiss Federal Council[155]:21 examined the concerns that bitcoin might be a pyramid scheme; it concluded that \"Since in the case of bitcoin the typical promises of profits are lacking, it cannot be assumed that bitcoin is a pyramid scheme.\" In July 2017, billionaire Howard Marks referred to bitcoin as a pyramid scheme.[156]\nOn 12 September 2017, Jamie Dimon, CEO of JP Morgan Chase, called bitcoin a \"fraud\" and said he would fire anyone in his firm caught trading it. Zero Hedge claimed that the same day Dimon made his statement, JP Morgan also purchased a large amount of bitcoins for its clients.[157]\nBitcoin has been labelled a speculative bubble by many including former Fed Chairman Alan Greenspan[158] and economist John Quiggin.[159] Nobel Memorial Prize laureate Robert Shiller said that bitcoin \"exhibited many of the characteristics of a speculative bubble\".[160] Journalist Matthew Boesler in 2013 rejected the speculative bubble label and saw bitcoin's quick rise in price as nothing more than normal economic forces at work.[161] Timothy B. Lee, in a 2013 piece for The Washington Post pointed out that the observed cycles of appreciation and depreciation don't correspond to the definition of speculative bubble.[136] On 14 March 2014, the American business magnate Warren Buffett said, \"Stay away from it. It's a mirage, basically.\"[162]\nTwo lead software developers of bitcoin, Gavin Andresen[163] and Mike Hearn,[164] have warned that bubbles may occur. David Andolfatto, a vice president at the Federal Reserve Bank of St. Louis, stated, \"Is bitcoin a bubble? Yes, if bubble is defined as a liquidity premium.\" According to Andolfatto, the price of bitcoin \"consists purely of a bubble,\" but he concedes that many assets \"have bubble component to their price\".[54]:21 Speculation in bitcoin has been compared to the tulip mania of seventeenth-century Holland. Comparisons have been made by the vice-president of the European Central Bank, V\u00edtor Const\u00e2ncio, by JPMorgan Chase chief Jamie Dimon,[165] by hedge fund manager Ken Griffin of Citadel,[166] and by former president of the Dutch Central Bank, Nout Wellink.[167] In 2013, Wellink remarked, \"This is worse than the tulip mania [...] At least then you got a tulip [at the end], now you get nothing.\"[167] On 13 September 2017, Jamie Dimon compared bitcoin to a bubble, saying it was only useful for drug dealers and countries like North Korea.[168] On 22 September 2017, a hedge fund named Blockswater subsequently accused JP Morgan of market manipulation and filed a market abuse complaint with Financial Supervisory Authority (Sweden).[169]\nLegal status, tax and regulation\nBecause of bitcoin's decentralized nature, nation-states cannot shut down the network or alter its technical rules.[174] However, the use of bitcoin can be criminalized, and shutting down exchanges and the peer-to-peer economy in a given country would constitute a \"de facto ban\".[175] The legal status of bitcoin varies substantially from country to country and is still undefined or changing in many of them. While some countries have explicitly allowed its use and trade, others have banned or restricted it. Regulations and bans that apply to bitcoin probably extend to similar cryptocurrency systems.[176]\nEnergy consumption\nBitcoin has been criticized for the amounts of electricity consumed by mining. As of 2015, The Economist estimated that even if all miners used modern facilities, the combined electricity consumption would be 166.7 megawatts (1.46 terawatt-hours per year).[100][177] As of November 2017, the global bitcoin mining activity was estimated[by whom?] to consume 3.4 gigawatts (30 TWh a year).[178]\nTo lower the costs, bitcoin miners have set up in places like Iceland where geothermal energy is cheap and cooling Arctic air is free.[81] Chinese bitcoin miners are known to use hydroelectric power in Tibet to reduce electricity costs.[179]\nCriminal activity\nThe use of bitcoin by criminals has attracted the attention of financial regulators, legislative bodies, law enforcement, and the media.[180] The FBI prepared an intelligence assessment,[181] the SEC has issued a pointed warning about investment schemes using virtual currencies,[180] and the US Senate held a hearing on virtual currencies in November 2013.[80]\nSeveral news outlets have asserted that the popularity of bitcoins hinges on the ability to use them to purchase illegal goods.[97][182] In 2014, researchers at the University of Kentucky found \"robust evidence that computer programming enthusiasts and illegal activity drive interest in bitcoin, and find limited or no support for political and investment motives.\"[183]\nIn popular culture\nIn September 2015, the establishment of the peer-reviewed academic journal Ledger (ISSN 2379-5980) was announced. It will cover studies of cryptocurrencies and related technologies, and is published by the University of Pittsburgh.[184][185] The journal encourages authors to digitally sign a file hash of submitted papers, which will then be timestamped into the bitcoin blockchain. Authors are also asked to include a personal bitcoin address in the first page of their papers.[186][187]\nThe documentary film, The Rise and Rise of Bitcoin (late 2014), features interviews with people who use bitcoin, such as a computer programmer and a drug dealer.[188]\nIn November 2017, the American sitcom, The Big Bang Theory, dedicated an episode to the topic of bitcoins titled, \"The Bitcoin Entanglement\". In the episode, after hearing the price of a bitcoin had risen to $5000, friends try to track down bitcoins they mined seven years earlier.[191]\nBook: Bitcoin", "title": "Bitcoin"}, {"text": "React allows developers to create large web-applications that use data and can change over time without reloading the page. It aims primarily to provide speed, simplicity, and scalability. React processes only user interfaces in applications. This corresponds to View in the Model-View-Controller (MVC) pattern, and can be used in combination with other JavaScript libraries or frameworks in MVC, such as AngularJS.[6]\nReact was created by Jordan Walke, a software engineer at Facebook. He was influenced by XHP, an HTML component framework for PHP.[7] It was first deployed on Facebook's newsfeed in 2011 and later on Instagram.com in 2012.[8] It was open-sourced at JSConf US in May 2013.\nReact Native, which enables native Android, iOS, and UWP development with React, was announced at Facebook's React.js Conf in February 2015 and open-sourced in March 2015.\nOn April 18, 2017, Facebook announced React Fiber, a new core algorithm of React framework library for building user interfaces.[9] React Fiber will become the foundation of any future improvements and feature development of the React framework.[10]\nThe following is a rudimentary example of how React can be used in html using JSX and the ECMAScript 2015 JavaScript syntax.\n<div id=\"myReactApp\"></div>\n\n<script type=\"text/babel\">\n  class Greeter extends React.Component { \n    render() { \n      return <h1>{this.props.greeting}</h1>\n    } \n  } \n\n  ReactDOM.render(<Greeter greeting=\"Hello World!\" />, document.getElementById('myReactApp'));\n</script>\nThe Greeter class is a React component that accepts a property greeting. The ReactDOM.render method creates an instance of the Greeter component, sets the greeting property to 'Hello World' and inserts the rendered component as a child element to the DOM element with id myReactApp.\nBabel will transpile the above ECMAScript 2015 class code to ES5 code to the following:\nReact.createElement(\n        \"h1\",\n        null,\n        this.props.greeting\n      );\nWhen displayed in a web browser the result will be\n<div id=\"myReactApp\">\n  <h1>Hello World!</h1>\n</div>\nTo use React to its greatest potential, Properties (or props), ideally a set of immutable values, are passed to a component's render function. A component should not directly modify any properties passed to it, but should be passed callback functions that instead modifies the store creating a single source of truth. This mechanism's promise is expressed as \"properties flow down; actions flow up\". The described mechanism is an architecture called Flux.[11][12] Many Flux alternatives have been created since its inception however the community has moved toward Redux.[13]\nAnother notable feature is the use of a \"virtual Document Object Model\", or \"virtual DOM\". React creates an in-memory data structure cache, computes the resulting differences, and then updates the browser's displayed DOM efficiently.[14] This allows the programmer to write code as if the entire page is rendered on each change, while the React libraries only render sub components that actually change.\nReact components are typically written in JSX, a JavaScript extension syntax allowing quoting of HTML and using HTML tag syntax to render subcomponents.[15] This is a React-specific grammar extension to JavaScript like the now-defunct E4X. HTML syntax is processed into JavaScript calls of the React framework. Developers may also write in pure JavaScript. JSX is similar to another extension syntax created by Facebook for PHP, XHP. JSX looks like regular HTML. An example of JSX code:\nMultiple elements need to be wrapped in a single container element like the <div> element shown above. As of React v16.0 it is now possible to return an array of elements, fragments and strings.[16]\nCustom attributes are supported in addition to HTML attributes. The custom attributes need to be added with the data- prefix. Custom attributes as of v16.0 are now passed through to the DOM.[17]\nJavaScript expressions (but not statements) can be used inside JSX with curly brackets {}:\nThe example above will render\n<div>\n  <h1>11</h1>\n</div>\nIf\u2013else statements cannot be used inside JSX but conditional expressions can be used instead. The example below will render { i === 1 ? 'true' : 'false' } as the string 'true' because i is equal to 1.\nNot only can you return strings via conditional operators you can also pass functions and fragments of code.\nThe example above if arr has values (of which it has 3) will render:\n<div>\n  <div>Section 1</div>\n  <div>Section 2</div>\n  <div>Section 3</div>\n</div>\nThe basic architecture of React applies beyond rendering HTML in the browser. For example, Facebook has dynamic charts that render to <canvas> tags,[18] and Netflix and PayPal use isomorphic loading to render identical HTML on both the server and client.[19][20]\nReact Native libraries were announced by Facebook in 2015,[21] providing the React architecture to native Android, iOS, and UWP[22] applications.\nProject status can be tracked via the core team discussion forum.[23] However major changes to React go through the Future of React repo, Issues and PR.[24][25] This enables the React community to provide feedback on new potential features, experimental APIs and JavaScript syntax improvements.\nThe status of the React sub-projects used to be available in the project wiki.[26]\nThe initial public release of React in May 2013 used a standard Apache License 2.0. In October 2014, React 0.12.0 replaced this with a 3-clause BSD license and added a separate PATENTS text file that permits usage of any Facebook patents related to the software:[29]\n\"The license granted hereunder will terminate, automatically and without notice, for anyone that makes any claim (including by filing any lawsuit, assertion or other action) alleging (a) direct, indirect, or contributory infringement or inducement to infringe any patent: (i) by Facebook or any of its subsidiaries or affiliates, whether or not such claim is related to the Software, (ii) by any party if such claim arises in whole or in part from any software, product or service of Facebook or any of its subsidiaries or affiliates, whether or not such claim is related to the Software, or (iii) by any party relating to the Software; or (b) that any right in any patent claim of Facebook is invalid or unenforceable.\"\nThis unconventional clause caused some controversy and debate in the React user community, because it could be interpreted to empower Facebook to revoke the license in many scenarios, for example, if Facebook sues the licensee prompting them to take \"other action\" by publishing the action on a blog or elsewhere. Many expressed concerns that Facebook could unfairly exploit the termination clause or that integrating React into a product might complicate a startup company's future acquisition.[30]\nBased on community feedback, Facebook updated the patent grant in April 2015 to be less ambiguous and more permissive:[31]\n\"The license granted hereunder will terminate, automatically and without notice, if you (or any of your subsidiaries, corporate affiliates or agents) initiate directly or indirectly, or take a direct financial interest in, any Patent Assertion: (i) against Facebook or any of its subsidiaries or corporate affiliates, (ii) against any party if such Patent Assertion arises in whole or in part from any software, technology, product or service of Facebook or any of its subsidiaries or corporate affiliates, or (iii) against any party relating to the Software. [...] A \"Patent Assertion\" is any lawsuit or other action alleging direct, indirect, or contributory infringement or inducement to infringe any patent, including a cross-claim or counterclaim.\"[32]\nThe Apache Software Foundation considered this licensing arrangement to be incompatible with its licensing policies, as it \"passes along risk to downstream consumers of our software imbalanced in favor of the licensor, not the licensee, thereby violating our Apache legal policy of being a universal donor\", and \"are not a subset of those found in the [Apache License 2.0], and they cannot be sublicensed as [Apache License 2.0].\"[33]. In August 2017, Facebook dismissed the Apache Foundation's downstream concerns and refused to reconsider their license[34][35], and, the following month, WordPress decided to switch their Gutenberg and Calypso projects away from React.[36]\nOn September 23, 2017, Facebook announced that the following week, it would re-license Flow, Jest, React, and Immutable.js under a standard MIT License; the company stated that React was \"the foundation of a broad ecosystem of open source software for the web\", and that they did not want to \"hold back forward progress for nontechnical reasons.\"[37]\nOn September 26, 2017, React 16.0.0 has been released with the MIT license.[38] The MIT license change has also been backported to the 15.x release line with React 15.6.2.[39]", "title": "React (JavaScript library)"}, {"text": "What is Ethereum?\nAccording to the Ethereum website, \u201cEthereum is a decentralized platform that runs smart contracts.\u201d This is an accurate summary but in my experience when first explaining Ethereum to friends, family, and strangers it helps to compare Ethereum to Bitcoin since a lot of people have at least heard about Bitcoin before. This beginner\u2019s guide should help those who are new to Ethereum to understand the high level differences between the two.\nComparison\nSimply put, Bitcoin can be described as digital money. Bitcoin has been around for eight years and is used to transfer money from one person to another. It is commonly used as a store of value and has been a critical way for the public to understand the concept of a decentralized digital currency.\nEthereum is different than Bitcoin in that it allows for smart contracts which can be described as highly programmable digital money. Imagine automatically sending money from one person to another but only when a certain set of conditions are met. For example an individual wants to purchase a home from another person. Traditionally there are multiple third parties involved in the exchange including lawyers and escrow agents which makes the process unnecessarily slow and expensive. With Ethereum, a piece of code could automatically transfer the home ownership to the buyer and the funds to the seller after a deal is agreed upon without needing a third party to execute on their behalf.\nThe potential for this is incredible! Think of the numerous applications that act as a third party to connect you with others based on some set logic (e.g. Uber, Airbnb, eBay). Many of the centralized systems we use today could be built in a decentralized manner on Ethereum. With Ethereum you can make these transactions trustless which opens up an entire world of decentralized applications. Decentralization is important because it eliminates single points of failure or control. This makes internal collusion and external attacks impractical. Decentralized platforms cut out the middlemen which ultimately leads to lower costs for the user. There are a few decentralized applications I am particularly excited about.\nIdentity\nThere are many websites a person can create a digital identity on (e.g. Facebook, Twitter, LinkedIn). This is cumbersome to manage and at the end of the day you are not in full control of your information as it is still owned by a centralized entity. With Ethereum you can have a decentralized identity management system like uPort that allows you to be in full control of your data. There is no centralized server that has access to your private data, can get hacked, edit your information, or get shut down.\nRight now in the US we have credit bureaus (e.g. Experian, TransUnion, Equifax) that other institutions like banks rely on to tell them your credit. Credit bureaus can put certain groups such as international and young people at a disadvantage. Lending Club, a peer to peer lending platform, addresses the problem of traditional financial services relying solely on FICO scores by offering additional data points like home ownership, income, and length of employment. Ethereum applications like uPort can go one step further by allowing you to control your own data, identity, and reputation.\nComputing Power / Storage\nConsider all of the spare computing power and storage a regular person might have on their computer. If it is not being used, then why not make it available to someone else? It is a similar concept to renting out a spare bedroom on Airbnb. An added benefit to using a decentralized application is that there are no centralized servers that are prone to censorship.\nThere are several projects in development to allow people to rent spare computing power and storage from those that have it. Filecoin allows people to rent out their computer storage to others and get paid for it. Similarly, Golem allows people to rent out their computing power. Ideas like these are not completely new. Since 2000, Folding@home has allowed volunteers to contribute spare computing power for scientific research at Stanford University. Now this concept can be monetized and applied to other industries, potentially lowering costs.\nSocial Media\nAkasha is a decentralized social media platform. There are no centralized servers so no single party has complete control over the content. This means that the platform is resistant to censorship. An added benefit of building a decentralized application for social media on Ethereum is that one can create a system that financially rewards high quality content. This is like Reddit but you can send small amounts of money to the poster instead of upvotes.\nRights Management\nDecentralized applications may be used to bring transparency to multiple industries. For example SingularDTV offers an entertainment rights management platform which allows transparent distribution of funds to the creators, investors, crew, actors, and others involved in a project. There is no centralized party which can prevent a certain group from getting access to their funds because the terms are enforced by code. Everyone will get paid according to the terms discussed up front and no third party is needed to mediate.\nManaging Companies\nA time consuming and often expensive aspect of starting a new company is allocating and managing shares. As companies grow and raise more funds they eventually need to issue and move shares around. Aragon is an example of a promising project that has an easy to use interface for managing the company\u2019s cap table and raising capital.\nRaising Capital\nLastly, one of the major use cases for Ethereum is decentralized fundraising from a global network of investors. Crowdsales lower the barrier to entry for developers working on high risk projects. Since Ethereum launched in July 2015 we have seen unprecedented amounts of funds raised for decentralized applications through crowdsales. Ethereum itself was funded through a crowdsale that raised $18 million in bitcoin and a project called The DAO raised $160 million. Some other notable crowdsales are shown below:\nFor more information about how these tokens work, refer to How to Raise Money on a Blockchain with a Token, Blockchain Token Securities Law Framework, and The difference between App Coins and Protocol Tokens.\nResources\nThis post only covered a handful of Ethereum\u2019s countless use cases. The space is constantly growing and innovating. Below are some links that may help you understand Ethereum further and keep up with the exciting news.\nUnderstanding Ethereum\nKeeping up with Ethereum", "title": "A beginner\u2019s guide to Ethereum"}, {"text": "Kubernetes (commonly referred to as \"K8s\"[3]) is an open-source system for automating deployment, scaling and management of containerized applications[4] that was originally designed by Google and donated to the Cloud Native Computing Foundation. It aims to provide a \"platform for automating deployment, scaling, and operations of application containers across clusters of hosts\".[5] It supports a range of container tools, including Docker.\n3 Architecture\nKubernetes (Greek for \"helmsman\" or \"pilot\") was founded by Joe Beda, Brendan Burns and Craig McLuckie,[6] was quickly joined by other Google engineers including Brian Grant and Tim Hockin, and was first announced by Google in mid-2014.[7] Its development and design are heavily influenced by Google's Borg system, [8][9] and many of the top contributors to the project previously worked on Borg. The original codename for Kubernetes within Google was Project Seven, a reference to a Star Trek character that is a 'friendlier' Borg.[10] The seven spokes on the wheel of the Kubernetes logo is a nod to that codename.\nKubernetes v1.0 was released on July 21, 2015.[11] Along with the Kubernetes v1.0 release, Google partnered with the Linux Foundation to form the Cloud Native Computing Foundation (CNCF)[12] and offered Kubernetes as a seed technology.\nRancher Labs includes a Kubernetes distribution in its Rancher container management platform.[13] Kubernetes is also being used by Red Hat for its OpenShift product,[14][15] CoreOS for its Tectonic product, and IBM for its IBM Cloud Container Service [16][17] and IBM Cloud Private product.[18][19][20] Oracle joined the Cloud Native Computing Foundation as a platinum member on September 13, 2017. Oracle open sourced a Kubernetes installer for Oracle Cloud Infrastructure and released Kubernetes on Oracle Linux.[21]\nKubernetes defines a set of building blocks (\"primitives\") which collectively provide mechanisms for deploying, maintaining, and scaling applications. The components which make up Kubernetes are designed to be loosely coupled and extensible so that it can meet a wide variety of different workloads. The extensibility is provided in large part by the Kubernetes API, which is used by internal components as well as extensions and containers running on Kubernetes.[22]\nThe basic scheduling unit in Kubernetes is called a \"pod\". It adds a higher level of abstraction to containerized components. A pod consists of one or more containers that are guaranteed to be co-located on the host machine and can share resources.[22] Each pod in Kubernetes is assigned a unique (within the cluster) IP address, which allows applications to use ports without the risk of conflict.[23] A pod can define a volume, such as a local disk directory or a network disk, and expose it to the containers in the pod.[24] Pods can be manually managed through the Kubernetes API, or their management can be delegated to a controller.[22]\nLabels and selectors[edit]\nKubernetes enables clients (users or internal components) to attach key-value pairs called \"labels\" to any API object in the system, such as pods and nodes. Correspondingly, \"label selectors\" are queries against labels that resolve to matching objects.[22]\nLabels and selectors are the primary grouping mechanism in Kubernetes, and are used to determine the components to which an operation applies.[25]\nFor example, if the Pods of an application have labels for a system tier (\"front-end\", \"back-end\", for example) and a release_track (\"canary\", \"production\", for example), then an operation on all of the \"back-end\" and \"canary\" nodes could use a label selector such as the following:[26]\ntier=back-end AND release_track=canary\nA controller is a reconciliation loop that drives actual cluster state toward the desired cluster state.[27] It does this by managing a set of pods. One kind of controller is a \"Replication Controller,\" which handles replication and scaling by running a specified number of copies of a pod across the cluster. It also handles creating replacement pods if the underlying node fails.[27] Other controllers that are part of the core Kubernetes system include a \"DaemonSet Controller\" for running exactly one pod on every machine (or some subset of machines), and a \"Job Controller\" for running pods that run to completion, e.g. as part of a batch job.[28] The set of pods that a controller manages is determined by label selectors that are part of the controller\u2019s definition.[26]\nA Kubernetes service is a set of pods that work together, such as one tier of a multi-tier application. The set of pods that constitute a service are defined by a label selector.[22] Kubernetes provides service discovery and request routing by assigning a stable IP address and DNS name to the service, and load balances traffic in a round-robin manner to network connections of that IP address among the pods matching the selector (even as failures cause the pods to move from machine to machine).[23] By default a service is exposed inside a cluster (e.g. back end pods might be grouped into a service, with requests from the front-end pods load-balanced among them), but a service can also be exposed outside a cluster (e.g. for clients to reach frontend pods)[29]\nKubernetes follows the master-slave architecture. The components of Kubernetes can be divided into those that manage an individual node and those that are part of the control plane.[22][30]\nThe Kubernetes Master is the main controlling unit of the cluster that manages its workload and directs communication across the system. The Kubernetes control plane consists of various components, each its own process, that can run both on a single master node or on multiple masters supporting high-availability clusters.[30] The various components of Kubernetes control plane are as follows:\netcd is a persistent, lightweight, distributed, key-value data store developed by CoreOS that reliably stores the configuration data of the cluster, representing the overall state of the cluster at any given point of time. Other components watch for changes to this store to bring themselves into the desired state.[30]\nThe API server is a key component and serves the Kubernetes API using JSON over HTTP, which provides both the internal and external interface to Kubernetes.[22][31] The API server processes and validates REST requests and updates state of the API objects in etcd, thereby allowing clients to configure workloads and containers across Worker nodes.\nThe scheduler is the pluggable component that selects which node an unscheduled pod (the basic entity managed by the scheduler) should run on based on resource availability. Scheduler tracks resource utilization on each node to ensure that workload is not scheduled in excess of the available resources. For this purpose, the scheduler must know the resource requirements, resource availability and a variety of other user-provided constraints and policy directives such as quality-of-service, affinity/anti-affinity requirements, data locality and so on. In essence, the scheduler\u2019s role is to match resource \"supply\" to workload \"demand\".[32]\nThe controller manager is the process that the core Kubernetes controllers like DaemonSet Controller and Replication Controller run in. The controllers communicate with the API server to create, update and delete the resources they manage (pods, service endpoints, etc.)[31]\nThe Node also known as Worker or Minion is the single machine (or virtual machine) where containers (workloads) are deployed. Every node in the cluster must run the container runtime (such as Docker), as well as the below-mentioned components, for communication with master for network configuration of these containers.\nKubelet is responsible for the running state of each node (that is, ensuring that all containers on the node are healthy). It takes care of starting, stopping, and maintaining application containers (organized into pods) as directed by the control plane.[22][33]\nKubelet monitors the state of a pod and if not in the desired state, the pod will be redeployed to the same node. The node status is relayed every few seconds via heartbeat messages to the master. Once the master detects a node failure, the Replication Controller observes this state change and launches pods on other healthy nodes.[citation needed]\nThe Kube-proxy is an implementation of a network proxy and a load balancer, and it supports the service abstraction along with other networking operation.[22] It is responsible for routing traffic to the appropriate container based on IP and port number of the incoming request.\ncAdvisor is an agent that monitors and gathers resource usage and performance metrics such as CPU, memory, file and network usage of containers on each node.", "title": "Kubernetes"}, {"text": "This post is part 1 in a 4-part series about Docker monitoring. Part 2 explores metrics that are available from Docker, part 3 covers the nuts and bolts of collecting those Docker metrics, and part 4 describes how the largest TV and radio outlet in the U.S. monitors Docker. This article dives into some of the new challenges Docker creates for infrastructure and application monitoring.\nYou have probably heard of Docker\u2014it is a young container technology with a ton of momentum. But if you haven\u2019t, you can think of containers as easily\u2014configured, lightweight VMs that start up fast, often in under one second. Containers are ideal for microservice architectures and for environments that scale rapidly or release often.\nDocker is becoming such an important technology that it is likely that your organization will begin working with Docker soon, if it has not already. When we explored real usage data, we found an explosion of Docker usage in production: it has grown 5x in the last 12 months.\nContainers address several important operational problems; that is why Docker is taking the infrastructure world by storm.\nBut there is a problem: containers come and go so frequently, and change so rapidly, that they can be an order of magnitude more difficult to monitor and understand than physical or virtual hosts. This article describes the Docker monitoring problem\u2014and solution\u2014in detail.\nWe hope that reading this article will help you fall in love with monitoring containers, despite the challenges. In our experience, if you approach monitoring in a way that works for containers\u2014whether or not you use them\u2014you will have great visibility into your infrastructure and applications.\nWhat is a container?\nA container is a lightweight virtual runtime. Its primary purpose is to provide software isolation. There are three environments commonly used to provide software isolation:\nphysical machine (heavyweight)\nvirtual machine (medium-weight)\ncontainer (lightweight)\nA significant architectural shift toward containers is underway, and as with any architectural shift, that means new operational challenges. The well-understood challenges include orchestration, networking, and configuration\u2014in fact there are many active software projects addressing these issues.\nThe significant operational challenge of monitoring containers is much less well-understood.\nDocker monitoring is crucial\nRunning software in production without monitoring is like driving without visibility: you have no idea if you\u2019re about to crash, or how to stay on the road.\nThe need for monitoring is well understood, so traditional monitoring solutions cover the traditional stack:\napplication performance monitoring instruments your custom code to identify and pinpoint bottlenecks or errors\ninfrastructure monitoring collects metrics about the host, such as CPU load and available memory\nGap in the stack\nHowever, as we describe later in this post, containers exist in a twilight zone somewhere between hosts and applications where neither application performance monitoring nor traditional infrastructure monitoring are effective. This creates a blind spot in your monitoring, which is a big problem for containers and for the companies that adopt them.\nA quick overview of containers\nIn order to understand why containers are a big problem for traditional monitoring tools, let\u2019s go deeper on what a container is and give some historical context.\nOriginal benefit: Security\nLightweight virtual runtimes have been around for a long time. Depending on the operating system, containers have been called different things: jail (BSD), zone (Solaris), cgroup (Linux), and more. As the name \u201cjail\u201d implies, containers were initially designed for security\u2014they provided runtime isolation without the overhead of full virtualization.\n\u00dcber-process or mini-host?\nA container provides a way to run software in isolation, and it is neither a process nor a host\u2014it exists somewhere on the continuum between. In the table below, note the differences between processes and hosts; containers are often similar to both of them.\nProcess\nContainer\nHost\nSpec\nSource\nDockerfile\nKickstart\nOn disk\n.TEXT\n/var/lib/docker\n/\nIn memory\nPID\nContainer ID\nHostname\nIn the network\nSocket\nveth*\neth*\nRuntime context\nserver core\nhost\ndata center\nIsolation\nmoderate: memory space, etc.\nprivate OS view: own PID space, file system, network interfaces\nfull: including own page caches and kernel\nContainers today\nAs mentioned, containers provide some (relative) security benefits with low overhead. But today there are two far more important reasons to use containers: they provide a pattern for scale, and an escape from dependency hell.\nModern benefits\nA pattern for scale\nUsing a container technology like Docker, it is easy to deploy new containers programmatically using projects/services such as Kubernetes or ECS. If you also design your systems to have a microservice architecture so that different pieces of your system may be swapped out or scaled up without affecting the rest, you\u2019ve got a great pattern for scale. The system can be elastic; growing and shrinking automatically with load, and releases may be rolled out without downtime.\nEscape from dependency hell\nThe second (and possibly more important) benefit of containers is that they free engineers from dependency hell.\nOnce upon a time, libraries were compiled directly into executables. This was good until libraries grew too large and started taking up too much scarce RAM.\nTo solve this problem, processes began to share libraries at runtime\u2014only loading them into memory once. This reduced memory consumption, but created painful dependency problems when the necessary library was not available at runtime, or when two processes required different versions of the same library. The gates of dependency hell were opened, but the savings in memory made the tradeoff worthwhile.\nToday there is a lot less memory pressure, but the legacy of dependency hell remains: the default way to build software today is with shared libraries. We kept the painful dependencies even when the original reason to use them subsided.\nTo partially address the pain created by dependencies, we created packages and package managers\u2014apt, yum, rvm, virtualenv, etc.\u2014to install groups of binaries that reliably work together. But packages required users to wait for upstream updates, which slowed the release cycle down so badly that crucial fixes and updates were held back for months or even years. This approach proved to be too slow for many companies.\nTo address the slow-release problem, people started to bundle their code and dependencies into /opt. Then came tools like omnibus to make self-contained packages. And now here we are back, full circle, to static binaries.\nToday, containers provide software engineers and ops engineers the best escape from dependency hell by packaging up an entire mini-host in a lightweight, isolated, virtual runtime that is unaffected by other software running on the same host\u2014all with a manifest that can be checked in to git and versioned just like code.\nContainer challenge: Massive operational complexity\nWe know that a container is basically a mini-host. Best practices for host ops are well-established, so you might suppose that container ops are basically the same\u2014but they are not.\nThe diagram above shows how a standard application stack has evolved over the past 15 years. (\u201cOff-the-shelf\u201d could represent your J2EE runtime or your database.)\nLeft: 15 years ago\nMiddle: about 7 years ago, virtualization with a service like EC2 gained wide adoption. This evolution allowed better resource utilization and near-instant host provisioning, but for an engineer few things changed\nRight: today a containerized stack running on virtualized hardware is gaining popularity\nFrom our vantage point at Datadog, we receive data from hundreds of thousands of hosts, which allows us to measure how many containers are running on each host in the wild today. In 2015, we\u2019re seeing that the median Docker-adopting company runs four containers simultaneously on each host. Given that containers tend to be shorter-lived than traditional hosts, the median VM runs nine containers in its life.\nMetrics Explosion\nBefore containers, you might monitor 150 metrics per Amazon EC2 host:\nComponent\n# Metrics\nOS (e.g. Linux)\n100\nOff-the-shelf component\n50\nTotal\n150\nNow let\u2019s add containers to the stack. For each container, assume you collect 50 metrics from the container itself, plus another 50 metrics reported by an off-the-shelf component running in the container. (This is a conservative number, as we see our customers collect many more.) In that case we would add 100 new metrics per container. So the number of metrics we will collect is:\nOS + (Containers per host * (Container + Off-the-shelf)) =\n100 + (4 * (50 + 50)) = 500 metrics per host\nChange acceleration\nBut it is not quite that simple.\nThe lifetime of a host is much longer than that of a container\u20144x longer on average. Rather than having a mix of short-lived and long-lived EC2 instances with median uptime measured in days, weeks or months, instead your median uptime for containers will be measured in minutes or hours.\nTo make matters more complex, new versions of containers are created and ready to deploy as fast as you can git commit. You\u2019ll find yourself rotating your container fleet on a daily basis.\nTo manage this, you\u2019ll likely want to use Kubernetes or AWS ECS to move from manual, imperative provisioning to autonomic, declarative provisioning. This allows you to say, for example, \u201cI need one container of type X per instance per zone, at all times,\u201d and your scheduler will make sure this is always the case. This kind of automation is necessary for modern container architectures, but opens the door to fresh new kinds of chaos.\nIn summary, with containers you\u2019ll be doing a lot more, a lot faster and you will need a modern monitoring solution to be prepared for this new reality.\nHost-centric monitoring\nIf your monitoring is centered around hosts, your world looks like Ptolemaic astronomy: complicated. It\u2019s pretty hard to account for the movement of planets this way.\nMoving to a cloud environment forces many companies to rethink host-centric monitoring because instances come and go, and different groups within their organization spin up new stacks with little advance notice.\nWhether or not you are using the cloud, if you add containers to your stack, your world gets much, much more complex. In fact, it gets so complex that host-centric monitoring tools simply can\u2019t explain your system, and you\u2019ll be left with two choices:\nTreat containers as hosts that come and go every few minutes. In this case your life is miserable because the monitoring system always thinks half of your infrastructure is on fire.\nDon\u2019t track containers at all. You see what happens in the operating system and the app, but everything in the middle is a gap, as discussed earlier.\nIf you\u2019re planning to monitor containers the same way as you\u2019ve monitored hosts before, you should expect a very painful ride.\nGoal: Simplify monitoring\nInstead we need a new approach, one that does not treat everything as a host.\nThe picture above represents Copernican astronomy. Compared with putting the earth at the center of the universe, Copernicus\u2019s radical suggestion is strikingly clear and simple.\nIf you forget about hosts and recenter your monitoring around layers and tags, the complexity will fall away and your operations will be sane and straightforward.\nNo Gaps\nTo avoid driving blind, you want your entire stack to be monitored from the top to the bottom, without gaps. If you\u2019re building on EC2, you probably use CloudWatch to monitor the VMs, infrastructure monitoring in the middle, and application performance monitoring at the top to measure throughput and help pinpoint problem areas in your code.\nFor monitoring layers to work, the key is that you must be able to see what\u2019s happening across the layers simultaneously, and determine how problems in one part of the stack ripple to the rest of the stack. For example, if you see slow response times in your application, but can\u2019t tell that it is being caused by a spike in IO at the VM layer, then your monitoring approach isn\u2019t helping you solve your problem.\nTo effectively monitor containers, you also need to tag (label) your containers. The good news is that you probably already use tags through AWS or server automation tools.\nBy centering your monitoring universe on tags, you can reorient from being imperative to declarative, which is analogous to how auto-scaling groups or Docker orchestration works. Rather than instructing your system to monitor a particular host or container, you can instruct your system to monitor everything that shares a common property (tag)\u2014for example, all containers located in the same availability zone.\nTags allow you to monitor your containers with powerful queries such as this (tags are bold):\nMonitor all Docker containers running image web in region us-west-2 across all availability zones that use more than 1.5x the average memory on c3.xlarge\nWith queryable tags you\u2019ll have a powerful system that makes monitoring your highly dynamic, container-based architecture straightforward and effective.\nConclusion\nBecause containers provide both an escape from software dependency hell and scaffolding for scalable software architectures, they are already becoming increasingly common in production.\nHowever, containers are typically used in large numbers and have a very short half-life, so they can easily increase operational complexity by an order of magnitude. Because of this, today many stacks that use containers do not monitor the containers themselves. This creates a huge blind spot and leaves the systems vulnerable to downtime.\nTherefore:\nMonitor all layers of your stack together, so that you can see what is happening everywhere, at the same time, with no gaps\nTag your containers so that you can monitor them as queryable sets rather than as individuals\nWith these guidelines in place, you should be ready for a painless containerized future.\nPart two of this series provides details about the key metrics that are available via Docker monitoring.\nSource Markdown for this post is available on GitHub. Questions, corrections, additions, etc.? Please let us know.", "title": "The Docker monitoring problem"}, {"text": "On the Google Compute Engine (GCE) platform, the default logging support targets Stackdriver Logging, which is described in detail in the Logging With Stackdriver Logging.\nThis article describes how to set up a cluster to ingest logs into Elasticsearch and view them using Kibana, as an alternative to Stackdriver Logging when running on GCE. Note that Elasticsearch and Kibana cannot be setup automatically in the Kubernetes cluster hosted on Google Kubernetes Engine, you have to deploy it manually.\nTo use Elasticsearch and Kibana for cluster logging, you should set the following environment variable as shown below when creating your cluster with kube-up.sh:\nKUBE_LOGGING_DESTINATION=elasticsearch\nYou should also ensure that KUBE_ENABLE_NODE_LOGGING=true (which is the default for the GCE platform).\nNow, when you create a cluster, a message will indicate that the Fluentd log collection daemons that run on each node will target Elasticsearch:\n$ cluster/kube-up.sh\n...\nProject: kubernetes-satnam\nZone: us-central1-b\n... calling kube-up\nProject: kubernetes-satnam\nZone: us-central1-b\n+++ Staging server tars to Google Storage: gs://kubernetes-staging-e6d0e81793/devel\n+++ kubernetes-server-linux-amd64.tar.gz uploaded (sha1 = 6987c098277871b6d69623141276924ab687f89d)\n+++ kubernetes-salt.tar.gz uploaded (sha1 = bdfc83ed6b60fa9e3bff9004b542cfc643464cd0)\nLooking for already existing resources\nStarting master and configuring firewalls\nCreated [https://www.googleapis.com/compute/v1/projects/kubernetes-satnam/zones/us-central1-b/disks/kubernetes-master-pd].\nNAME                 ZONE          SIZE_GB TYPE   STATUS\nkubernetes-master-pd us-central1-b 20      pd-ssd READY\nCreated [https://www.googleapis.com/compute/v1/projects/kubernetes-satnam/regions/us-central1/addresses/kubernetes-master-ip].\n+++ Logging using Fluentd to elasticsearch\nThe per-node Fluentd pods, the Elasticsearch pods, and the Kibana pods should all be running in the kube-system namespace soon after the cluster comes to life.\n$ kubectl get pods --namespace=kube-system\nNAME                                           READY     STATUS    RESTARTS   AGE\nelasticsearch-logging-v1-78nog                 1/1       Running   0          2h\nelasticsearch-logging-v1-nj2nb                 1/1       Running   0          2h\nfluentd-elasticsearch-kubernetes-node-5oq0     1/1       Running   0          2h\nfluentd-elasticsearch-kubernetes-node-6896     1/1       Running   0          2h\nfluentd-elasticsearch-kubernetes-node-l1ds     1/1       Running   0          2h\nfluentd-elasticsearch-kubernetes-node-lz9j     1/1       Running   0          2h\nkibana-logging-v1-bhpo8                        1/1       Running   0          2h\nkube-dns-v3-7r1l9                              3/3       Running   0          2h\nmonitoring-heapster-v4-yl332                   1/1       Running   1          2h\nmonitoring-influx-grafana-v1-o79xf             2/2       Running   0          2h\nThe fluentd-elasticsearch pods gather logs from each node and send them to the elasticsearch-logging pods, which are part of a service named elasticsearch-logging. These Elasticsearch pods store the logs and expose them via a REST API. The kibana-logging pod provides a web UI for reading the logs stored in Elasticsearch, and is part of a service named kibana-logging.\nThe Elasticsearch and Kibana services are both in the kube-system namespace and are not directly exposed via a publicly reachable IP address. To reach them, follow the instructions for Accessing services running in a cluster.\nIf you try accessing the elasticsearch-logging service in your browser, you\u2019ll see a status page that looks something like this:\nYou can now type Elasticsearch queries directly into the browser, if you\u2019d like. See Elasticsearch\u2019s documentation for more details on how to do so.\nAlternatively, you can view your cluster\u2019s logs using Kibana (again using the instructions for accessing a service running in the cluster). The first time you visit the Kibana URL you will be presented with a page that asks you to configure your view of the ingested logs. Select the option for timeseries values and select @timestamp. On the following page select the Discover tab and then you should be able to see the ingested logs. You can set the refresh interval to 5 seconds to have the logs regularly refreshed.\nHere is a typical view of ingested logs from the Kibana viewer:\nKibana opens up all sorts of powerful options for exploring your logs! For some ideas on how to dig into it, check out Kibana\u2019s documentation.\nCreate an Issue Edit this Page", "title": "Logging Using Elasticsearch and Kibana"}, {"text": "Apache Spark is an open-source cluster-computing framework. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\nApache Spark has as its architectural foundation the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.[2] In Spark 1.x, the RDD was the primary application programming interface (API), but as of Spark 2.x use of the Dataset API is encouraged[3] even though the RDD API is not deprecated.[4][5] The RDD technology still underlies the Dataset API.[6]\nSpark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.[7]\nSpark facilitates the implementation of both iterative algorithms, that visit their data set multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated database-style querying of data. The latency of such applications may be reduced by several orders of magnitude compared to a MapReduce implementation (as was common in Apache Hadoop stacks).[2][8] Among the class of iterative algorithms are the training algorithms for machine learning systems, which formed the initial impetus for developing Apache Spark.[9]\nApache Spark requires a cluster manager and a distributed storage system. For cluster management, Spark supports standalone (native Spark cluster), Hadoop YARN, or Apache Mesos.[10] For distributed storage, Spark can interface with a wide variety, including Hadoop Distributed File System (HDFS),[11] MapR File System (MapR-FS),[12] Cassandra,[13] OpenStack Swift, Amazon S3, Kudu, or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per CPU core.\nSpark Core is the foundation of the overall project. It provides distributed task dispatching, scheduling, and basic I/O functionalities, exposed through an application programming interface (for Java, Python, Scala, and R) centered on the RDD abstraction (the Java API is available for other JVM languages, but is also usable for some other non-JVM languages, such as Julia,[14] that can connect to the JVM). This interface mirrors a functional/higher-order model of programming: a \"driver\" program invokes parallel operations such as map, filter or reduce on an RDD by passing a function to Spark, which then schedules the function's execution in parallel on the cluster.[2] These operations, and additional ones such as joins, take RDDs as input and produce new RDDs. RDDs are immutable and their operations are lazy; fault-tolerance is achieved by keeping track of the \"lineage\" of each RDD (the sequence of operations that produced it) so that it can be reconstructed in the case of data loss. RDDs can contain any type of Python, Java, or Scala objects.\nAside from the RDD-oriented functional style of programming, Spark provides two restricted forms of shared variables: broadcast variables reference read-only data that needs to be available on all nodes, while accumulators can be used to program reductions in an imperative style.[2]\nA typical example of RDD-centric functional programming is the following Scala program that computes the frequencies of all words occurring in a set of text files and prints the most common ones. Each map, flatMap (a variant of map) and reduceByKey takes an anonymous function that performs a simple operation on a single data item (or a pair of items), and applies its argument to transform an RDD into a new RDD.\nval conf = new SparkConf().setAppName(\"wiki_test\") // create a spark config object\nval sc = new SparkContext(conf) // Create a spark context\nval data = sc.textFile(\"/path/to/somedir\") // Read files from \"somedir\" into an RDD of (filename, content) pairs.\nval tokens = data.flatMap(_.split(\" \")) // Split each file into a list of tokens (words).\nval wordFreq = tokens.map((_, 1)).reduceByKey(_ + _) // Add a count of one to each token, then sum the counts per word type.\nwordFreq.sortBy(s => -s._2).map(x => (x._2, x._1)).top(10) // Get the top 10 words. Swap word and count to sort by count.\nSpark SQL is a component on top of Spark Core that introduced a data abstraction called DataFrames,[a] which provides support for structured and semi-structured data. Spark SQL provides a domain-specific language (DSL) to manipulate DataFrames in Scala, Java, or Python. It also provides SQL language support, with command-line interfaces and ODBC/JDBC server. Although DataFrames lack the compile-time type-checking afforded by RDDs, as of Spark 2.0, the strongly typed DataSet is fully supported by Spark SQL as well.\nimport org.apache.spark.sql.SQLContext\n\nval url = \"jdbc:mysql://yourIP:yourPort/test?user=yourUsername;password=yourPassword\" // URL for your database server.\nval sqlContext = new org.apache.spark.sql.SQLContext(sc) // Create a sql context object\n\nval df = sqlContext\n  .read\n  .format(\"jdbc\")\n  .option(\"url\", url)\n  .option(\"dbtable\", \"people\")\n  .load()\n\ndf.printSchema() // Looks the schema of this DataFrame.\nval countsByAge = df.groupBy(\"age\").count() // Counts people by age\nSpark Streaming leverages Spark Core's fast scheduling capability to perform streaming analytics. It ingests data in mini-batches and performs RDD transformations on those mini-batches of data. This design enables the same set of application code written for batch analytics to be used in streaming analytics, thus facilitating easy implementation of lambda architecture.[16][17] However, this convenience comes with the penalty of latency equal to the mini-batch duration. Other streaming data engines that process event by event rather than in mini-batches include Storm and the streaming component of Flink.[18] Spark Streaming has support built-in to consume from Kafka, Flume, Twitter, ZeroMQ, Kinesis, and TCP/IP sockets.[19]\nIn Spark 2.x, a separate technology based on Datasets, called Structured Streaming, that has a higher-level interface is also provided to support streaming.[20]\nSpark MLlib is a distributed machine learning framework on top of Spark Core that, due in large part to the distributed memory-based Spark architecture, is as much as nine times as fast as the disk-based implementation used by Apache Mahout (according to benchmarks done by the MLlib developers against the Alternating Least Squares (ALS) implementations, and before Mahout itself gained a Spark interface), and scales better than Vowpal Wabbit.[21] Many common machine learning and statistical algorithms have been implemented and are shipped with MLlib which simplifies large scale machine learning pipelines, including:\nGraphX is a distributed graph processing framework on top of Apache Spark. Because it is based on RDDs, which are immutable, graphs are immutable and thus GraphX is unsuitable for graphs that need to be updated, let alone in a transactional manner like a graph database.[23] GraphX provides two separate APIs for implementation of massively parallel algorithms (such as PageRank): a Pregel abstraction, and a more general MapReduce style API.[24] Unlike its predecessor Bagel, which was formally deprecated in Spark 1.6, GraphX has full support for property graphs (graphs where properties can be attached to edges and vertices).[25]\nGraphX can be viewed as being the Spark in-memory version of Apache Giraph, which utilized Hadoop disk-based MapReduce.[26]\nLike Apache Spark, GraphX initially started as a research project at UC Berkeley's AMPLab and Databricks, and was later donated to the Apache Software Foundation and the Spark project.[27]\nSpark was initially started by Matei Zaharia at UC Berkeley's AMPLab in 2009, and open sourced in 2010 under a BSD license.\nIn 2013, the project was donated to the Apache Software Foundation and switched its license to Apache 2.0. In February 2014, Spark became a Top-Level Apache Project.[28]\nSpark had in excess of 1000 contributors in 2015,[30] making it one of the most active projects in the Apache Software Foundation[31] and one of the most active open source big data projects.\nGiven the popularity of the platform by 2014, paid programs like General Assembly and free fellowships like The Data Incubator have started offering customized training courses[32]\nVersion\nOriginal release date\nLatest version\nRelease date\nOld version, no longer supported: 0.5\n2012-06-12\n0.5.1\n2012-10-07\nOld version, no longer supported: 0.6\n2012-10-14\n0.6.2\n2013-02-07[33]\nOld version, no longer supported: 0.7\n2013-02-27\n0.7.3\n2013-07-16\nOld version, no longer supported: 0.8\n2013-09-25\n0.8.1\n2013-12-19\nOld version, no longer supported: 0.9\n2014-02-02\n0.9.2\n2014-07-23\nOld version, no longer supported: 1.0\n2014-05-30\n1.0.2\n2014-08-05\nOld version, no longer supported: 1.1\n2014-09-11\n1.1.1\n2014-11-26\nOld version, no longer supported: 1.2\n2014-12-18\n1.2.2\n2015-04-17\nOld version, no longer supported: 1.3\n2015-03-13\n1.3.1\n2015-04-17\nOld version, no longer supported: 1.4\n2015-06-11\n1.4.1\n2015-07-15\nOld version, no longer supported: 1.5\n2015-09-09\n1.5.2\n2015-11-09\nOlder version, yet still supported: 1.6\n2016-01-04\n1.6.3\n2016-11-07\nOlder version, yet still supported: 2.0\n2016-07-26\n2.0.2\n2016-11-14\nOlder version, yet still supported: 2.1\n2016-12-28\n2.1.1\n2017-05-02\nCurrent stable version: 2.2\n2017-07-11\n2.2.1\n2017-12-01\nLegend: Old version Older version, still supported Latest version Latest preview version Future release", "title": "Apache Spark"}, {"text": "More\nFor Amazon (AMZN), 2017 was another blockbuster stretch \u2014 and not just because it was Yahoo Finance\u2019s Company of the Year. The Seattle tech giant saw its stock soar nearly 58%, driven by strong revenues, the $13.7 billion acquisition of Whole Foods Market and the popularity of voice assistant Alexa.\nFar less buzzed about was Amazon Web Services\u2019 ongoing hot streak. In the first three quarters of 2017, AWS generated $12.3 billion in revenues \u2014 up from nearly $8.7 billion in revenues during the same period in 2016 \u2014 and continued to have cloud computing cornered with a 35% market share. And during its sixth-annual re: Invent conference in Las Vegas this November, AWS released over 60 new features and product updates, including Alexa for Business.\n\u201cAWS is focused on giving companies the opportunity to iterate and innovate faster and more easily than anywhere else,\u201d Andy Jassy, CEO of AWS, told Yahoo Finance. \u201cOur significantly broader functionality and ecosystem, as well as performance maturity, are why so many more companies continue to choose AWS as their long-term, strategic technology partner.\u201d\nJassy contends there\u2019s much more untapped potential in the cloud computing space as more companies traverse the big shift from managing their own data servers to storing and crunching their data remotely in the cloud.\nOne piece of cloud-based technology Jassy remains particularly bullish about is voice-activated applications including Alexa, which runs off AWS. On its own, Alexa is the undisputed market leader, with hundreds of third-party Alexa-enabled devices on the market now, alongside competitors such as Google Home, Microsoft\u2019s Cortana and Apple\u2019s Siri. But the AWS chief executive foresees a future where voice becomes even more intertwined in people\u2019s everyday lives as voice technology becomes more sophisticated and one day able to perform more tasks, like say, booking an entire trip from beginning to end.\n\u201cWhen we first started being able to use applications on the phone, tapping a few times felt pretty handy,\u201d Jassy adds. \u201cThen you used an application on something like Alexa, a voice-driven application, and it actually seems awfully inconvenient to have to tap three or four times. So we\u2019re going to see an explosion of voice applications over time, as well.\u201d\nMoving at a breakneck pace\nAWS wasn\u2019t always a surefire moneymaker or the go-to cloud computing provider for millions of businesses.\nWhen it launched in 2006, pundits at the time wondered why CEO Jeff Bezos strayed so far from the company\u2019s retail roots and building data centers that stored and processed information for other businesses. In the BusinessWeek cover story \u201cJeff Bezos\u2019 Risky Bet\u201d from October 2006, one analyst griped that then-new investments like AWS were \u201cmore of a distraction than anything else.\u201d\nFast-forward to the present, and AWS is obviously more than a mere \u201cdistraction.\u201d Although AWS isn\u2019t as flashy as Amazon\u2019s booming retail business, it nonetheless accounted for 10% of Amazon\u2019s overall revenues during the first three quarters of 2017.\nRolling out myriad new features and updates at a breakneck pace is a large part of the reason AWS stays on top.\nAccording to Jassy, the cloud computing business follows a 90-10 rule when it comes to development: 90% of what AWS builds is in direct response to what clients ask for, while the remaining 10% is what he calls a \u201cstrategic interpretation\u201d of customer needs.\nOne example of a feature AWS launched as a direct result of many AWS customers asking for it is Amazon Redshift, a service rolled out in February 2013 that basically serves up a fully automated virtual warehouse that processes and spits out data at brisk speeds. According to Jassy, Redshift became one of the fastest-growing services in the history of AWS, although he did not specify exactly what Redshift growth looks like.", "title": "Amazon Web Services CEO: We're going to see an 'explosion' of voice apps"}]